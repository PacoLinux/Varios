\documentstyle[emlines]{report}
\begin{document}\title{Direct compilation of high level
 languages for Multi-media instruction-sets}
\author{Paul Cockshott}

\maketitle
\tableofcontents
\chapter{Multi-media instruction-sets}
A number of widely used contemporary processors have instructionset extensions for
improved performance in multi-media applications. The aim is to allow operations 
to proceed on multiple pixels each clock cycle. Such instructionsets have been
incorporated both in specialist DSP chips like the Texas C62xx\cite{texas} and in
general purpose CPU chips like the Intel IA32\cite{IA32} or the AMD K6\cite{K6}.
\section{The SIMD model}
These instructionset extensions are typically based on the Single Instruction-stream Multiple
Data-stream (SIMD) model in which a single instruction causes the same mathematical operation
to be carried out on many operands, or pairs of operands at the same time.
The SIMD model was originally developed in the context of large scale parallel
machines such as the ICL Distributed Array Processor or the Connection Machine.
In these systems a single control processor broadcast an instruction to thousands of
single bit wide dataprocessors causing each to perform the same action in lockstep.
These early SIMD processors exhibited massive data-parallelism, but, with each data processor
having its own private memory and data-bus they were  bulky machines involving multiple
boards each carrying multiple memory chip, data-processor chip pairs.
Whilst they used single bit processors, the SIMD model is not dependent on this. It
can also be implemented with multiple 8-bit, 16-bit or 32-bit data processors.

The incorporation of SIMD technology onto modern general purpose microprocessors
is on a more modest scale than were the pioneering efforts. 
For reasons of economy the SIMD engine has to be
included on the same die as the rest of the CPU. This immediately constrains the
degree of parallelism that can be obtained. The constraint does not arise
 from the difficulties of incorporating large numbers of simple processing units.
With contemporary feature sizes, one could fit more than a thousand 1-bit processors
on a die. Instead the degree of parallelism is constrained by the width of the CPU to
memory data path. 

The SIMD model provides for all data-processors to simultaneously   
transfer words of data between internal registers and corresponding locations in  their memory banks.
Thus with $n$ data-processors each using $w$-bit words one needs a path to
memory of $nw$ bits.\label{datapath}
If a CPU chip has a 64-bit memory bus then it could support 64 1-bit SIMD 
data-processors, or 8 8-bit data-processors, 2 32-bit processors, etc.

For bulk data operations, such as those involved in image processing, the relevant
memory bus is the off chip bus. For algorithms that can exploit some degree
of data locality the relevant bus would be that linking the CPU to the on-chip
cache, and the degree of parallelism possible would be constrained by the
width of the cache lines used.

Whilst memory access paths constrain the degree of parallelism possible, the
large numbers of logic gates available on modern dies, allows the complexity of
the individual data-processors to be raised. Instead of performing simple
 1-bit arithmetic, they  do arithmetic on multi-bit integers and floating
point numbers.

As a combined result of these altered constraints we find that SIMD 
instructions for multi-media applications have parallelism levels of between
32 bits (Texas C62xx) and 128 bits (Intel PIII), and the supported data
types range from 8-bit integers to 32-bit floating point numbers. 
\section{The MMX register architecture}

The MMX architectural extensions were introduced by Intel in late
models of the Pentium processor. They have subsequently been incorporated
in the PII and PIII processors from Intel and in compatible 
chips produced by AMD, Cyrix and others. They can now be considered
part of the baseline architecture of any contemporary PC.

\begin{figure}
\input regs.pic
\caption{The Intel IA32 with MMX register architecture}\label{fig:mmxreg}
\end{figure}

The data registers available for computational purposes on processors
incorporating the MMX architecture are shown in Figure \ref{fig:mmxreg}.
The original IA32 architecture had eight general purpose registers and
an eight deep stack of floating point registers. When designing
the multi-media extensions to the instructionset, Intel wanted
to ensure that no new state bits were added to the process model.
Adding new state bits would have made CPU's with the 
extensions incompatible with existing operating systems, as these
would not have saved the additional state on a task switch. Instead,
Intel added 8 new virtual 64-bit registers which are aliased onto the
existing floating point stack. These new multimedia registers, {\tt mm0..mm7},
use state bits already allocated to the Floating Point Unit(FPU), and are
thus saved when an operating system saves the state of the FPU.  

The MMX instructions share addressing mode formats with the
instructions used for the general purpose registers. The 3-bit
register identification fields inherited from the previous instructions
are now used to index the eight multi-media rather than the eight
general purpose registers.
The existing addressing modes for memory operands are also carried
over allowing the full gamut of base and index address modes
to be applied to the loading and storing of MMX operands.


\section{MMX data-types}

The MMX registers support 4 data formats as shown in Figure \ref{fig:mmxfmt}. A register can hold
a single 64-bit {\ tt QWORD}, a pair of 32-bit {\tt DWORDS}, 4 16-bit {\tt WORDS} or 8 {\tt BYTES}.
Within these formats the data-types shown in Table \ref{mmxtypes} are supported.
\begin{figure}
\input mmxfmt.pic
\caption{The MMX data formats}\label{fig:mmxfmt}
\end{figure}
The saturated data-types require special comment.
They are designed to handle a circumstance that arises frequently in image processing when
using pixels represented as integers: that of constraining the result of some arithmetic
operation to be within the meaningful bounds of the integer representation.



\begin{table}
\caption{MMX data-types}\label{mmxtypes}
\begin{tabular}{lcccc}
Format &signed& unsigned &signed saturated &unsigned saturated\\
BYTE&yes&yes&yes&yes\\
WORD&yes&yes&yes&yes\\
DWORD&yes&yes&no&no\\
\end{tabular} 
\end{table}

%Consider the problem of applying the following vertical edge sharpening convolution kernel
%to an image represented as signed bytes.
%\begin{tabular}{c|c|c}
%-0.25&0.75&-0.25\\\hline
%-0.5&1.5&-0.5\\\hline
%-0.25&0.75&-0.25\\
%\end{tabular}
%Since the kernel is unitary, that is, its elements sum to 1, it produces no overall
%change in the contrast of the image. The image, being represented in signed bytes,
%will have pixels in the range -128..127, with -128 representing black and 127 
%representing white.
%The effect of the convolution should be to enhance the contrast on any vertical
%lines or vertical edges.



%Now consider the effect of applying the kernel to the 3x4 pixel pattern
% \begin{tabular}{c|c|c|c}
%0&-70&-70&0\\\hline
%0&-70&-70&0\\\hline
%0&-70&-70&0\\
%\end{tabular}
%which represents a 2 pixel wide  dark grey vertical line on a mid grey background.
%The intended effect should be to enhance the contrast between the line and the
%background.

%If we perform the calculations for the convolution using real arithemetic\footnote{
%For speed we might use 16 bit integers representing the convolution as
%\begin{tabular}{c|c|c}
%-1&3&-1\\\hline
%-2&6&-2\\\hline
%-1&3&-1\\
%\end{tabular} followed by a shift right 2 places to normalise the
%result, but the argument above would still hold.
%}, the
%pixels $p$ representing the dark grey line ( the -70's) are mapped to $p'=3\times -70 + (-1\times -70)=-140$.
%The snag is that -140 is less than the smallest signed 8 bit integer.
%The only 'sensible' value that can be assigned to $p'$ would be -128=black.
%If we simply converted -140 to an 8 bit signed value by truncation, we would obtain
%1110011 binary or 115 decimal. The dark line would have been mapped to a light
%line contrary to intention.

Suppose we are adding two images represented as arrays of bytes in the
range 0..255 with 0 representing black and 255 white. It is possible that the results may be greater than 255.
For example 200+175 = 375 but in 8 bit binary
{\tt\begin{tabular}{lr}
&11001000\\
+&10101111\\\hline
=&1 01110111\\
\end{tabular}}
droping the leading 1 we get $01110111=119$, which is dimmer than either of the 
original pixels. The only sensible answer in this case would have been 255,
representing white.

To avoid such errors, image processing code using 8 bit values has to put in
tests to check if values are going out of range, and force all out of range values
to the appropriate extremes of the ranges. This inevitably slows down the 
computation of inner loops. Besides introducing additional instructions, the
tests involve conditional branches and pipeline stalls.

The MMX seeks to obviate this by providing packed saturated data types with
appropriate arithmetic operations over them. These use hardware to ensure that
the numbers remain in range.

The combined effect of the use of packed data and saturated types can
be to produce a significant increase in code density and performance.
   
\begin{figure}
\begin{verbatim}

main()
{
	unsigned char v1[LEN],v2[LEN],v3[LEN];
	int i,j,t;
		for (j=0;j<LEN;j++){
			t=v2[j]+v1[j];
			v3[j]=(unsigned char)(t>255?255:t);
		}
			
}
ASSEMBLER
        xor       edx, edx                                      ; 9.8
                                                                ;
                                ; LOE edx ebx ebp esi edi esp dl dh bl bh
$B1$3:                          ; Preds $B1$5 $B1$2
        mov       eax, edx                                      ; 10.9
        lea       ecx, DWORD PTR [esp]                          ; 10.6
        movzx     ecx, BYTE PTR [eax+ecx]                       ; 10.6
        mov       DWORD PTR [esp+19200], edi                    ;
        lea       edi, DWORD PTR [esp+6400]                     ; 10.12
        movzx     edi, BYTE PTR [eax+edi]                       ; 10.12
        add       ecx, edi                                      ; 10.12
        cmp       ecx, 255                                      ; 11.26
        mov       edi, DWORD PTR [esp+19200]                    ;
        jle       $B1$5         ; Prob 16%                      ; 11.26
                                ; LOE eax edx ecx ebx ebp esi edi esp al ah dl dh cl ch bl bh
$B1$4:                          ; Preds $B1$3
        mov       ecx, 255                                      ; 11.26
                                ; LOE eax edx ecx ebx ebp esi edi esp al ah dl dh cl ch bl bh
$B1$5:                          ; Preds $B1$3 $B1$4
        inc       edx                                           ; 9.18
        cmp       edx, 6400                                     ; 9.3
        mov       DWORD PTR [esp+19200], edi                    ;
        lea       edi, DWORD PTR [esp+12800]                    ; 11.4
        mov       BYTE PTR [eax+edi], cl                        ; 11.4
        mov       edi, DWORD PTR [esp+19200]                    ;
        jl        $B1$3         ; Prob 80%                      ; 9.3


\end{verbatim}
\caption{C code to add two images and corresponding assembler for the
inner loop. Code compiled on the Intel C compiler version 4.0.}\label{fig:imageadd}
\end{figure} 



Consider the  C code in figure \ref{fig:imageadd} to add 2 images pointed to by {\tt p1} and
{\tt p2}, storing the result in the image pointed to by {\tt p3}.
The code includes a check to prevent overflow.
Compiled into assembler code by the Visual C++ compiler the resulting
assembler code has 18 instructions in the inner loop.
The potential acceleration due to the MMX can be seen by
comparing it with the following hand coded assembler inner loop:

\begin{verbatim}
l1: movq mm0,[esi+ebp-LEN]    ; load 8 bytes
    paddusb mm0,[esi+ebp-2*LEN] ; packed unsigned add bytes
    movq [esi+ebp-3*LEN],mm0    ; store 8 byte result
    add esi,8	                ; inc dest pntr
    loop l1                     ; repeat for the rest of the array
   \end{verbatim}
The example assumes that {\tt p1,p2,p3 } are stored in
registers {\tt esi,edx,edi} for the duration of
the loop.
Only 5 instructions are used in the whole loop,
compared to 18 for the compiled C code. 
Furthermore, the MMX code processes 8 times
as much data per iteration, 
thus requiring only 0.625 instructions per byte
processed. The compiled code thus executes
29 times as many instructions to perform
the same task.
Whilst some
of this can be put down to the superiority of
hand assembled versus automatically compiled code,
the combination of the SIMD model and the saturated
arithmetic are obviously a major factor. 
\section{3D Now!}
The original MMX instructions introduced by Intel were targeted
at increasing the performance of 2D image processing, giving their
biggest performance boost for images of byte-pixels. The typical
operations in 3D graphics, perspective transformations, ray tracing,
rendering etc, tend to rely upon floating point data representation.
Certain high 2D image processing operations requiring high accuracy
such as high precision stereo matching can also be  implemented using floating point data.
Both Intel and AMD have seen the need to provide for these data representations.
AMD responded first with the 3D Now! instructions, then Intel introduced
the Steaming SIMD instructions which we discuss in the next section.

The basic IA32 architecture already provides support for 32-bit and
64-bit IEEE floating point instructions using the FPU stack. However,
64-bit floating point numbers are poor candidates for parallelism in view
of the datapath limitations described in section \ref{datapath}.
\begin{figure}
\input 3dnow.pic
\caption{The AMD 3DNOW! extensions add 32 bit floating point data to
the types that can be handled in MMX registers}\label{fig:3DNOW}
\end{figure}

AMD provided a straight forward extension of the MMX whereby an
additional data-type, the pair of 32 bit floats shown in figure \ref{fig:3DNOW},
could be operated on.
Type conversion operations are provided to convert between pairs of 32-bit
integers and 32-bit floats.

The range of operators supported includes instructions for the rapid computation
of reciprocals and square roots - relevant to the computation of 
Euclidean norms in 3D-space.

Another significant extension with 3D Now, copied in the Streaming
SIMD extensions is the ability to prefetch data into the cache
prior to its use.
This is potentially useful in any loop operating on an array of data.
 For instance the loop in the previous section could be acclerated by
 inserting the marked prefetch instructions.
\begin{verbatim}
l1: movq mm0,[esi]    ; load 8 bytes
    add esi,8         ; inc src pntr
    prefetch [esi]    ; ensure that the next 8 bytes worth are in the cache
    paddusb mm0,[edx] ; packed unsigned add bytes
    add edx,8         ; inc src pntr
    prefetch [edx]    ; preload 8 bytes from other array
    movq [edi],mm0    ; store 8 byte result
    add edi,8         ; inc dest pntr
    prefetchw [edi]   ; set up the cache ready for writing 8 bytes of data
\end{verbatim}
The instruction count rises, despite this  performance goes up since  loads into the cache
are initiated 6 instructions prior to the data being needed. This allows the loading
of the cache to be overlapped with useful instructions rather than forcing calculations
to stall whilst the load takes place.
\section{Streaming SIMD}
Intel produced their own functional equivalent to AMD's 3DNOW instructionset with the
Pentium III processor. They called the new instructions Streaming SIMD.
As with 3DNOW, the Streaming SIMD instructions combine cache prefetching
techniques with parallel operations on short vectors of 32 bit floating point
operands.
\begin{figure}
\input xmm.pic
\caption{The Streaming SIMD extensions add additional 32 bit floating point 
vector registers}\label{fig:xmmpic}
\end{figure}

The most significant difference is to the model of machine state. Whilst
the original MMX instructions and 3DNOW add no new state to the machine
architecture, Streaming SIMD introduces additional registers. Eight
new 128-bit registers (XMM0..7) are introduced.
The addition of new state means that operating systems have to be
modified to ensure that xmm registers are saved during context switches.
Intel provided a driver to do this for Microsoft Windows NT 4.0, Windows
98 and subsequent Windows releases have this support built in.

The default format for the XMM registers is a 4-tuple of 32 bit floating
point numbers. Instructions are provided to perform parallel addition, multiplication,
subtraction and division on these 4-tuples. 
Two very useful extensions to this format are provided. \begin{enumerate}
\item A set of boolean operations are provided that treat the registers
as 128 bit words, useful for operations on bitmps.
\item Scalar operations are provided that operate only on the lower
32 bits of the register. This allows the XMM registers to be used for
conventional single precision floating point arithmetic. Whilst the
pre-existing Intel FPU instructions support single precision arithmetic,
the original FPU is based on a reverse Polish stack architecture. This
scheme does not fit well with the register allocation schemes used in 
some compilers. The existence of what are effectively eight scalar floating
point registers can lead to more efficient floating point code. 


\end{enumerate}
The scalar and vector uses of the XMM registers are contrasted in
table \ref{tab:xmm}. A special move instruction (MOVSS) is provided
to load or store the least significant 32 bits of an XMM register.


\begin{table}
\caption{The XMM registers support both scalar and vector arithmetic}\label{tab:xmm}
{\bf Vector addition}

ADDPS xmm0,xmm1
\begin{tabular}{lrrrrr}
xmm0&1.2&1.3&1.4&1.5&\\
xmm1&2.0&4.0&6.0&8.0&+\\\hline
xmm0&3.2&5.3&7.4&9.5&

\end{tabular}

{\bf Scalar addition}

ADDSS xmm0,xmm1
\begin{tabular}{lrrrrr}
xmm0&1.2&1.3&1.4&1.5&\\
xmm1&2.0&4.0&6.0&8.0&+\\\hline
xmm0&1.2&1.3&1.4&9.5&

\end{tabular}
\end{table}
\subsection{Cache optimisation}
The Streaming side of the Streaming SIMD extensions is concerned with optimising
the use of the cache.
The extensions will typically be used with large collections of data, too large
to fit into the cache. If an application were adding two vectors of a million
floating point registers using standard instructions, the 4MB of results would 
pollute the cache. 
This cache pollution can be avoided using the {\em non-temporal} store instructions,
MOVNTPS and MOVNTQ operating on the XMM and MM registers respectively.

A PREFETCH instruction is provided to preload data into the cache. This is more sophisticated than
 the equivalent 3DNOW! instruction described above.
The AMD instruction applies to all cache levels. The Intel variant allows the 
programmer to specify which levels of cache are to be preloaded.

Whereas all previous IA32 load and store instructions had operated
equally well on aligned and unaligned data. The Streaming SIMD extensions
introduces special load and store instructions to operate on aligned
128 bit words. General purpose load and store instructions capable of handling
unaligned data are retained.

\chapter{Possible programming techniques for MMX and Streaming SIMD}
In this chapterIwill look at  possible techniques for programming
the MMX hardware. In order to illustrate the techniquesIwill use
a sample program that adds two arrays of  bytes storing the
result in a third. The arrays
will be 6400 bytes long, and the operation will be repeated 100,000 times.

Timings for the program are given for a 233Mhz Sony Vaio.
The test program therefore requires 640,000,000 add operations.

The program is used purely for illustrative purposes, it is not
realistic as a benchmark of general performance on the MMX.
\section{Direct use of assembler code}
With instructionsets as complex as those incorporated into the latest
Intel and AMD processors, careful hand-written assembler language routines
produce the highest quality machine code.

Microsoft's MASM assembler supports the extended instructionset as does
the free assembler NASM. The latter has the advantage of running on both Linux and
Windows, and provides support for MMX, 3DNOW! and SIMD instructions.

In the absence of better tools, direct coding of inner loops
as assembler routines is the obvious course to pursue where
run-time performance requirements demand it.
Disadvantages of using assembler are well known:
\begin{enumerate}
\item It is not portable between processors. A program written in assembler
to use the AMD extensions will not run on an Intel processor nor,
{\em a-fortiori}, on an Alpha.
\item It requires the programmer to have an in-depth knowledge of the
underlying machine architecture, which only a small proportion of
programmers now have.
\item Productivity in terms of programmer time spent to implement
a given algorithm is lower than in high level languages.
\item The programmer must further master the low level linkage
and procedure call conventions of the high level language used
for the rest of the application.
\item Programmers have to master additional program development tools.
\end{enumerate} 

All of these militate against widespread use.
The fact that the AMD extensions can only be programmed in assembler may have
been a factor limiting their practical use.
\subsection{The example program}
\begin{figure}
\input tests/asm/vecadd.asm
\caption{Assembler version of the test program}\label{fig:asm}
\end{figure}
\begin{figure}
\input tests/C/seradd.c 
\caption{C version of the test program}\label{fig:vecaddC}
\end{figure}

The assembler version of the example program is shown in figure \ref{fig:asm}.
It runs in 4.01 seconds on the test machine, a 233Mhz Pentium II, a throughput of 160 million byte arithetic
operations per second.

The C version is shown in figure \ref{fig:vecaddC}. It compiled with the Intel C compiler it runs in 82 seconds on the test machine,
a performance of around 8 million arithmetic operations per second. Thus the assembler
version using MMX is about 20 times as fast as the C version.

\section{Use of assembler intrinsics}
Intel supply a C compiler that has low level extensions allowing
the extended instructions to be used.
Intel terms these extensions `assembler intrinsics'.
For example the ADDPS instruction which adds 4 packed single precision
floating point numbers is mirrored by the Intel C/C++ Compiler Intrinsic Equivalent
\begin{verbatim}
__m128 _mm_add_ps(__m128 a, __m128 b)
\end{verbatim}which adds the four SP FP values of a and b.

Syntactically these look like C functions but they are translated
one for one into equivalent assembler instructions.
The use of assembler intrinsics simplifies the process
of developing MMX code, in that programmers use a single
tool - the C compiler, and do not need to concern themselves
with low level linkage issues. However, the other disadvantages of
assembler coding remain. 

\begin{enumerate}
\item It is still not portable between processors.

\item It still requires the programmer to have an in-depth knowledge of the
underlying machine architecture.
\item Productivity is unlikely to be higher than with assembler.

\end{enumerate}
\section{Use of C++ classes}
The Intel C compiler comes with a set of C++ classes that correspond to the fundamental
types supported by the MMX and SIMD instructionsets.
For instance, type Iu8vec8 is a vector of 8 unsigned 8 bit integers, Is32vec2 a vector
of 2 signed 32 bit integers, etc. The basic arithmetic operators for addition, subtraction,
multiplication and division are then overloaded to support these vector types.

\begin{figure}
\input tests/C/vecadd.cpp 
\caption{C++ version of the test program}\label{fig:seraddC}
\end{figure}
Figure \ref{vecaddC} shows the example program implemented in C++ using the Intel
SIMD class Iu8vec8.
The SIMD classes do a good job of presenting the underlying capabilities
of the architecture within the context of the C language.
The code produced is also efficient, the example program in C++ runs in
4.56 seconds on the test machine, a performance of 140 million byte operations per second.
However, it has to be born in mind that the C++ code is not portable to
other processors. The compiler always generates MMX or SIMD instructions for
the classes. If run on a 486 processor these would be illegal. The C++ code
build around these classes, whilst it has a higher level of expression
than assembler intrinsics, is no more portable.

%\section{Image processing libraries}


%\section{Disadvantages of the techniques}
\begin{figure}
\begin{verbatim}
program vecadd;
type byte=0..255;
var v1,v2,v3:array[0..6399]of byte;
    i:integer;
begin
     for i:= 1 to 100000 do v3:=v1 + v2;
end.
\end{verbatim}
\caption{Example program in High Performance Pascal}
\label{fig:HPPexample}
\end{figure}
\begin{table}
\caption{Comparative performance of different versions of example program}
\begin{tabular}{lr}
Version& Time in seconds\\\hline
Assembler using MMX&4.95\\
C++ using MMX classes&5.00\\
High Performance Pascal to MMX instructionset&9.88\\

High Performance Pascal to 486 instructionset&75.41\\
C&87.88\\
\end{tabular}

\end{table}

\section{High level language expressions of data parallelism}
High level language notations for expressing data parallelism have
a long history, dating back to APL in the early '60s\cite{Iverson62}.
The  key concept here is the systematic overloading of all scalar operators
to work on arrays. 
Given some type $t$ let us denote an array whose elements are of type $t$ as having
type $t[]$. Then if we have a binary operator $\omega:(t \otimes t)\rightarrow t$ defined  
on type $t$, in languages derived from APL we automatically have an operator 
 $\omega:(t[]\otimes t[])\rightarrow t[]$.

 Thus if $x,y$ are arrays of
integers $k=x+y$ is the array of integers where $k_i = x_i+y_i$.

The basic concept is simple, there are complications to do with the
semantics of operations between arrays of different lengths and different
dimensions, but Iverson provides a consistent treatment of these.

The most recent languages to be built round this model are J, an
interpretive language\cite{jamanual}, and F\cite{Fmanual} a
modernised Fortran.
In principle though  any language with
array types can be extended in a similar way.
Unlike the SIMD classes provided by the Intel C++ compiler, the
APL type extensions are machine independent. They can be
implemented using scalar instructions or using the SIMD
model. The only difference is speed.

I have chosen this model as the basis for our experiments in
compiling high level language code to SIMD instructionsets.
 Our objectives in these experiments were to:
\begin{enumerate}
\item Provide a means of expressing data parallel operations
that was independent of the instructionset available.
\item To provide a mechanism for automatically generating
good machine code for data parallel operations.
\item To provide a means by which the code generator can be
automatically retargeted to different SIMD and non-SIMD 
instructionsets based on a formal description of these
instructionsets.
\item To provide a toolkit that was operating system and
machine independent.
\item To produce at least one demonstration compiler for
at least one language with data-parallelism to evaluate the
technique.
\item To produce at least two code generators one for a
scalar instructionset and one for a SIMD instructionset.
  
\end{enumerate}

For initial experiment I have chosen the base 486 and the Pentium
with MMX as the two instructionsets to be targeted.
An extended version of pascal with operators overloaded
in the APL style is used as the source language.
This is provisionally called High Performance Pascal.

\begin{figure}
\input overview.pic
\caption{System overview}\label{fig:overview}
\end{figure}

The structure of the experimental system is shown in figure \ref{fig:overview}.
It is complex.
Consider first the path followed from a source file {\tt foo.pas}, shown in the
upper right of the diagram, to an executable file {\tt foo.exe}, in the lower center
of the diagram. The phases that it goes through are
\begin{itemize}
\item[i.] The source file is parsed by a java class Hppc.class and results
in an internal data structure, an ilcg tree, which is basically a semantic
tree for the program. The classes from which this is built are from package {\tt ilcg.tree}.
\item[ii.] The resulting ilcg tree is walked over by a class that encapsulates
the semantics of the target machine's instructionset; in this case Pentium.class.
The output of this process is an assembler file {\tt foo.asm}.
\item[iii.] This is then fed through an appropriate assembler and linker, assumed
to be externally provided to generate an executable.
\end{itemize}

The key components on this path are Hppc.class and Pentium.class
Hppc is a simple, hand writen, recursive descent parser,
nothing much interesting about this. Pentium.class has a 
much more complicated genesis.
It is produced from a file Pentium.ilc, in a notation ILCG, which gives a semantic
description of the Pentium's instructionset. This is processed
by an application ILCG which builds the source file Pentium.java.

The application ILCG is built out of two main components: a collection
of classes that comprise a parser for the language ILCG, and a 
class which walks parse trees for ILCG programs and outputs
code generators. The parser classes are automatically generated
from a definition of the grammar of ILCG using the parser
generator Sable\cite{sable}. The walker WalkGen walks a parse
tree for an ILCG file and builds a walker for semantic trees. 

In what follows I provide a description of the notation
ILCG, a description of the semantic trees taken as input
to the code generators and then describe the strategy used in the generated
code generators to translate these trees to assembler code.




 



\chapter{ILCG as a notation for describing instuctionsets}
The purpose of ILCG to 
mediate between CPU instruction sets and high level language programs.
It poth provides a representation to which compilers can translate
a variety of source level programming languages and also a notation for
defining the semantics of CPU instructions.


Its purpose is to act as an input to two types of programs:
\begin{enumerate}\item ILCG structures produced by a HLL compiler are input to 
an automatically constructed code generator,
working on the syntax matching principles described in \cite{graham80}. This then
generates equivalent sequences of assembler statements.
\item
\end{enumerate}
It is assumed that all store allocation appart from register spillage has already been accomplished.

It is itself intended to be machine independent in that it abstracts from the instruction
model of the machine on which it is being executed. However, it will assume certain
things about the machine. It will assume that certain basic types are supported and that
the machine is addressed at the byte level.

We further assume that all type conversions, dereferences etc have to be made
absolutely explicit.
Since the notation is intended primarily to be machine generated we are not particularly
concerned with human readability, and thus use a prefix notation.

In what followsIwill designate terminals of the language in bold thus {\bf octet}
and nonterminal in sloping font thus {\sl word8}.
\section{Supported types}
\subsection{Data formats}
The data in a memory can be distinguished initially in terms of the number of bits
in the individually addressable chunks. The addressable chunks are assumed to be the
powers of two from 3 to 7, so we thus have as allowed formats:{\sl word8, word16, word32, word64,
word128}.
These are treated as non terminals in the grammar of ILCG.

When data is being explicitly operated on without regard to its type, we have terminals which
stand for these formats: {\bf octet, halfword, word, doubleword, quadword}.
\subsection{Typed formats}
Each of these underlying formats can contain information of different types, either
signed or unsigned integers, floats etc.

We thus allow the following integer types as terminals in the language:{\bf
int8, uint8, int16, uint16, int32, uint32, int64, uint64 }to stand for signed and
unsigned integers of the appropriate lengths.

The integers are logically grouped into {\sl signed} and {\sl unsigned}.
As non terminal types they are represented as 
{\sl byte, short, integer, long} and
{\sl ubyte, ushort, uinteger, ulong}.

Floating point numbers are either assumed to be 32 bit or 64 bit with 32 bit numbers
given the nonterminal symbols {\sl float,double}. If we wish to specify a particular
representation of floats of doubles we can use the terminals {\bf ieee32, ieee64}.



\subsection{Ref types}
A value can be a reference to another type. Thus an integer when used as
an address of a 64 bit floating point number would be a {\bf ref ieee64 }.
Ref types include registers. An integer register would be a {\bf ref int32}
when holding an integer, a {\bf ref ref int32} when holding the address
of an integer etc.
\section{Supported operations}
\subsection{Type coercions}
In order to prevent the $n^2 $ problem associated with conversions
between $n$ different types, ILCG provides a limited subset of 
allowed expansions, which, by composition allow any conversion requested
without minimal loss of accuracy.
The compressions necessarily involve a loss of accuracy.

The allowed conversions are summarised in the tables \ref{expansions}
and \ref{contractions}.

The syntax for the type conversions is C style so we have
for example {\tt (ieee64) int32} to represent a conversion
of an 32 bit integer to a 64 bit real.


\begin{table}\caption{Range expanding coercions}\label{expansions}
\center
\begin{tabular}{lll}
INPUT& OUTPUT \\
byte & integer\\
ubyte & integer\\
short &integer\\
ushort& integer\\
uinteger & double\\
integer & double \\
float & double \\
long & double \\
\end{tabular}
\end{table}
\begin{table}\caption{Range contracting coercions}\label{contractions}
\center
\begin{tabular}{ll}
INPUT& OUTPUT \\
integer & byte\\
integer & ubyte\\
integer &short\\
integer &ushort\\
double & uinteger\\
  double& integer\\
double & float \\
double & long \\
\end{tabular}
\end{table}
\subsection{Arithmetic}
The allowed arithmetic operations are addition, multiplication,
subtraction, 
division and remainder with operator symboles {\bf +,*,-,div , mod}.. 
The arithmetic is assumed to take place at the precisions 
of 32 bit integer and 64 bit reals.

The syntax is prefix with bracketing. Thus 
the infix operation $ 3+5 \div 7$ would be
represented as {\bf +(3 div (5 7))}.
\subsection{Memory}
Memory is explicitly represented. All accesses to memory are
represented by array operations on a predefined array {\bf mem}.
Thus location 100 in memory is represented as {\bf mem(100)}.
The type of such an expression is {\sl address}.
It can be cast to a reference type of a given format.
Thus we could have
{\bf

(ref int32)mem(100)
}

\subsection{ Assignment }
We have a set of storage  operators corresponding to the
word lengths supported. These  have the form of 
infix operators. The size of the store being performed
depends on the size of the right hand side.
A valid storage statement might be 
{\bf

(ref octet)mem( 299) :=(int8) 99

}
The first argument is always a reference and the second 
argument a value of the appropriate format.

If the left hand side is a format the right hand side
must be a value of the appropriate size.
If the left hand side is an explicit type rather than
a format, the right hand side must have the same type.
\subsection{ Dereferencing}
Dereferencing is done explicitly when a value other than a literal is required.
There is a dereference operator, which converts a reference 
into the value that it references.
 A valid load expression might be:
{\bf

(octet)$\uparrow$  ( (ref octet)mem(99))

}
The argument to the load operator must be a reference.

\section{Machine description}
Ilcg can be used to describe the semantics of machine instructions.
A machine description typically consists of a set of register declarations
followed by a set of instruction formats and a set of operations.
This approach works well only with machines that have an orthogonal
instruction set, ie, those that allow addressing modes and operators
to be combined in an independent manner.
\subsection{Registers }
When entering machine descriptions in ilcg registers can be declared
along with their type hence
{\bf

register word EBX assembles['ebx'] ;

reserved register word ESP assembles['esp'];
 

}
would declare {\bf EBX} to be of type {\bf ref word}.

\subsubsection{Aliasing}
A register can be declared to be a sub-field of another register,
hence we could write 
{\bf

 alias register octet AL = EAX(0:7) assembles['al']; 

 alias register octet BL = EBX(0:7) assembles['bl'];


}
to indicate that {\bf BL} occupies the bottom 8 bits of register {\bf EBX}.
In this notation bit zero is taken to be the least significant bit of a value.
There are assumed to be two pregiven registers {\bf FP, GP} that
are used by compilers to point to  areas of memory.
These can be aliased to a particular real register.{\bf

register word EBP assembles['ebp'] ;

alias register word FP = EBP(0:31) assembles ['ebp'];

}

Additional registers may be reserved, indicating that the code generator
must not use them to hold temporary values:

{\bf

reserved register word ESP assembles['esp'];
}
 
\subsection{Register sets}
A set of registers that are used in the same way by the instructionset
can be defined.
{\bf

pattern reg means [$ EBP| EBX|ESI|EDI|ECX |EAX|EDX|ESP$]  ;

pattern breg means[$ AL|AH|BL|BH|CL|CH|DL|DH$];
}

All registers in an register set should be of the same length.


\subsection{Register Arrays}
Some machine designs have regular arrays of registers.
Rather than have these exhaustively enumerated it is convenient to
have a means of providing an array of registers.
This can be declared as:


{\bf

register  vector(8)doubleword MM assembles['MM'i] ;

}
 
 
This declares the symbol MMX to stand for the entire MMX register
set. It implicitly defines how the register names are to be
printed in the assembly language by defining an indexing variable
i that is used in the assembly language definition.

We also need a syntax for explicitly identifying individual
registers in the set. This is done by using
the dyadic subscript operator:
{\bf

subscript(MM,2)

}
which would be of type {\bf ref doubleword}.
\subsection{Register Stacks}
Whilst some machines have registers organised as an array,
another class of machines, those oriented around postfix instructionsets,
have register stacks.

The ilcg syntax allows register stacks to be declared:



{\bf

register stack (8)ieee64 FP assembles[ ' '] ;


}

Two access operations are supported on stacks:
\paragraph{PUSH} is a void dyadic operator taking a stack of type ref $t$ as first argument
and a value of  type $t$ as the second argument. Thus we might have:
{ \bf

PUSH(FP,$\uparrow$mem(20))
}

\paragraph{POP} is a monadic operator returning $t$ on stacks of type $t$. So we might have
{\bf

mem(20):=POP(FP)
} 
%In addition there are two predicates on stacks that can be used in pattern pre-conditions.
%\paragraph{FULL} is a monadic boolean operator on stacks.
%\paragraph{EMPTY} is a monadic boolean operator on stacks.
\subsection{Instruction formats}

An instruction format is an abstraction over a class of concrete instructions.
It abstracts over particular operations
 and types thereof
whilst specifying how arguments can be combined.
{\bf

instruction pattern 

RR( operator op, anyreg r1, anyreg r2, int t)

        means[r1:=(t) op( $\uparrow$((ref t) r1),$\uparrow$((ref t) r2))]    

	assembles[op ' ' r1 ',' r2];


}
In the above example, we specify a register to register instruction format
that uses the first register as a source and a destination whilst the second
register is only a destination. The 
 result is returned in register r1.


We might however wish to have a more powerful abstraction, which was capable of
taking more abstract apecifications for its arguments. For example, many machines
allow arguments to instructions to be addressing modes that can be either
registers or memory references. For us to be able to specify this in an instruction
format we need to be able to provide grammer non-terminals as arguments
to the instruction formats.

For example we might want to be able to say

{\bf

instruction  pattern 

RRM(operator op, reg r1, maddrmode rm, int t)

        means [r1:=(t) op( $\uparrow$((ref t)r1),$\uparrow$((ref t) rm))]

        assembles[op ' ' r1 ',' rm ] ;}

This implies that addrmode and reg must be non terminals.
Since the non terminals required by different machines will vary, there
must be a means of declaring such non-terminals in ilcg.

An example would be:
{
\bf 

 


 pattern regindirf(reg r) 

	means[$\uparrow$(r) ]
	assembles[ r ];

pattern baseplusoffsetf(reg r, signed s) 

	means[+(  $\uparrow$(r) ,const s)]
	assembles[  r '+' s ];

pattern addrform means[baseplusoffsetf$|$ regindirf];

 

pattern maddrmode(addrform f) 

means[mem(f) ] assembles[ '[' f ']' ];

}
This gives us a way of including non terminals as parameters to patterns.


\section{The package ilcg.tree}
The package ilcg.tree provides java classes to support the code generation process. These fall into two broad 
categories, classes which implement the interface {\tt ilcg.tree.Node}, and classes
which are descended from {\tt ilcg.tree.Walker}.

The classes implementing the {\tt Node} interface are used to build ILCG trees. These classes are
in 1 to 1 correspondance with the constructs of the ILCG grammar. Thus for any expression defining
the semantics of a machine instruction in ILCG there is a corresponding tree structure which
matches it.


\begin{table}\begin{tabular}{|l|}
\hline
Implementing Classes for Node:\\ \hline
 
{\tt     Assign, Block, Cast, CharLit, Dyad, For, Format, Goto, If},
\\{\tt Integer, Label, Location,
     Memref, Monad, Op, Real,}
 \\{\tt Register, RegisterStack, Statement, Type  }\\ \hline
\end{tabular}
\end{table}

These trees can either be directly constructed by a Java program, or can be 
loaded from a {\tt .tree} file, which is basically a postfix flattened binary
encoding of the tree. Such {\tt .tree} files can be generated by front
ends written in languages other than Java.

\chapter{The ILCG code-generator generator}
%\chapter{Transformation strategies for data parallelism}
\chapter{The grammer for processor definition}
\input{ilcggram}
\input{/cpus/pentium.ilc}


\begin{thebibliography}{------------}


\bibitem{K6}Advanced Micro Devices, 3DNow!
Technology
Manual, 1999.



\bibitem{graham80} Susan L. Graham, Table Driven Code Generation, IEEE Computer, Vol 13, No. 8,
August 1980, pp 25..37.
\bibitem{IA32}Intel, Intel Architecture
Software Developer’s
Manual
Volumes  1 and 2, 1999.


\bibitem{sable} \' Etienne Gagnon , SABLECC, AN OBJECT-ORIENTED
                     COMPILER FRAMEWORK,
                                   School of Computer Science
                                   McGill University, Montreal ,
                                          March 1998. 
\bibitem{texas} Texas Instruments, TMS320C62xx CPU and Instruction Set Reference Guide, 1998.
\end{thebibliography}

\end{document}
